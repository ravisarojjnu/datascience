{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "visualization using PCA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPf4zT5Tzw4dpHke4Cx1mXe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravisarojjnu/datascience/blob/master/visualization_using_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9q9OW8XAMlC",
        "colab_type": "text"
      },
      "source": [
        "#### Dimensionality reduction and visualization using PCA\n",
        "\n",
        "# Why we need Dimensionality reduction :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlyueCpr_79v",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   Reduces the time and storage space required\n",
        "*   Removal of multi-collinearity improves the interpretation of the parameters of the machine learning model\n",
        "\n",
        "*   It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D. This is because in general we need to deal with very high dimensional data i.e. data with 100s and 1000s of dimensions so in that case it wont be possible to visualize that data and to work upon such data we may not have enough computation power as well.\n",
        "*   It avoids the curse of dimensionality.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u15SWbH_d_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbv7b8ssAAfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxQf1qQ0BDSA",
        "colab_type": "text"
      },
      "source": [
        "## Advantages of PCA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzF0KT31BL7k",
        "colab_type": "text"
      },
      "source": [
        "* PCA removes correlated features. After implementing the PCA on your dataset, all the Principal Components are independent of one another. There is no correlation among them.\n",
        "* verfitting mainly occurs when there are too many variables in the dataset. So, PCA helps in overcoming the overfitting issue by reducing the number of features.\n",
        "* It is very hard to visualize and understand the data in high dimensions. PCA transforms a high dimensional data to low dimensional data (2 or 3 dimension) so that it can be visualized easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO9N088ZBiNM",
        "colab_type": "text"
      },
      "source": [
        "## Limitations of PCA\n",
        "* PCA cannot be used for sparse data . For sparse data we use SVD .\n",
        "* If features are completely uncorrelated, the lower dimensional representation obtained using PCA would not preserve much of the variance in the original data and hence would be useless .PCA works fine if a subset of features are correlated . Before we apply PCA we try to remove outliers .\n",
        "* PCA assumes that the principal components are orthogonal.\n",
        "* Since PCA is a Feature extraction technique so the new features which are formed donot make any sense i.e. are not interpretable."
      ]
    }
  ]
}